\subsection{Temporal Correlation}
\label{sec:temporal_correlation}

When data is generated by using a Markov chain method, it is temporally
correlated which can be seen in the autocorrelation function given by equation
\cite{Janke2002}:
\begin{equation}
  \label{eq:autocorrelation}
  A(k) = \frac{\langle \mathcal{O}_i \mathcal{O}_{i+k}\rangle - \langle\mathcal{O}_i\rangle\langle\mathcal{O}_i\rangle}{\langle\mathcal{O}_i^2\rangle- \langle\mathcal{O}_i\rangle\langle\mathcal{O}_i\rangle}
\end{equation}

where $\mathcal{O}_i$ is the observable at time $i$ for example the energy or
magnetization. The autocorrelation function is a measure of how correlated the
data is at a given time step. If the data is uncorrelated the autocorrelation
function will be zero for all $k$. For large time steps $k$ the autocorrelation
function $A(k)$ will decay exponentially as:

\begin{equation}
  \label{eq:autocorrelation_decay}
  A(k) \xrightarrow{k\rightarrow\infty} A(0)e^{-k/\tau_{\mathcal{O}, exp}}
\end{equation}

where $\tau_{\mathcal{O}, exp}$ is the exponential autocorrelation time. The
autocorrelation function also contains some other modes because of which $A(k)$
does not decay exponentially for all $k$. 

The influence of the autocorrelation time is particularly important for phase
transitions. Near the critical point, the autocorrelation time
$\tau_{\mathcal{O},exp}$ scales in infinite volume limit as \cite{Janke2002}:

\begin{equation}
  \label{eq:autocorrelation_scaling}
  \tau_{\mathcal{O}, exp} \sim \xi^z
\end{equation}

where $z$ is the dynamical critical exponent greater than 0. $\xi$ is the
spacial correlation length which diverges at the critical point as $\xi \sim
|T-T_c|^{-\nu} \rightarrow \infty$ when $T\rightarrow T_c$. This implies that
the autocorrelation time diverges at the critical point as $\tau_{\mathcal{O},
exp} \sim |T-T_c|^{-\nu z}$. This effect is very large at local dynamics ($z
\approx 2$). This leads to a phenomenon called critical slowing down
\cite{Schneider1974,Heuer1992} resulting in a large increase in the simulation
time required to generate uncorrelated data. The issue of critical slowing down
can be avoided by using cluster algorithms \cite{Wolff1989}.


\subsubsection{Estimators}

Estimators are used to estimate the expectation value of an observable
$\mathcal{O}$ from a set of $N$ measurements $\mathcal{O}_k$ of the observable
$\mathcal{O}$ as:

\begin{equation}
  \label{eq:estimator}
  \langle\mathcal{O}\rangle = \sum_{\sigma} \mathcal{O}(\sigma)P^{eq}(\sigma) \approx \overline{\mathcal{O}} =  \frac{1}{N}\sum_{k=1}^{N}\mathcal{O}_k
\end{equation}

where $\sigma$ is the microstate of the system and $P^{eq}(\sigma)$ is the
equilibrium probability distribution of the microstates. The estimator
$\overline{\mathcal{O}}$ is an unbiased estimator of the expectation value of
the observable $\mathcal{O}$. In case of Markov chain Monte Carlo simulations,
the observable $\mathcal{O}$ is a quantity like energy or magnetization which is
measured at each time step $k$ of the simulation with N as the total number of
measurement sweeps. The estimator $\overline{\mathcal{O}}$ is only valid after a
sufficiently long thermalization period necessary to reach the equilibrium state
after starting from a random initial state.

It is important to differentiate between the estimator $\overline{\mathcal{O}}$
and the expectation value $\langle\mathcal{O}\rangle$. The expectation value
$\langle\mathcal{O}\rangle$ is the true value of the observable which is often
unknown. The estimator $\overline{\mathcal{O}}$ is an approximation of the
former. In contrast to the expectation value $\langle\mathcal{O}\rangle$ being a
constant, the estimator $\overline{\mathcal{O}}$ is a random variable which
fluctuates around the expectation value $\langle\mathcal{O}\rangle$ for finite
values of $N$. 

From a single simulation, we can only obtain a single value of the estimator
$\overline{\mathcal{O}}$. It might seem that we need to run multiple simulations
to obtain statistical error of the estimator. However, it is possible to obtain
the statistical error of the estimator $\overline{\mathcal{O}}$ from a single
simulation. We can express the variance of the estimator
$\overline{\mathcal{O}}$ as:

\begin{align}
  \sigma^2_{\mathcal{O}} &= \langle\overline{\mathcal{O}}^2\rangle - \langle\overline{\mathcal{O}}\rangle^2 \\
  &= \frac{1}{N^2}\sum_{k=1}^{N}\langle\mathcal{O}_k^2\rangle - \langle\mathcal{O}_k\rangle^2 + \frac{1}{N^2}\sum_{k\neq l}^{N}(\langle\mathcal{O}_k\mathcal{O}_l\rangle- \langle\mathcal{O}_k\rangle\langle\mathcal{O}_l\rangle)
  \label{eq:variance_estimator}
\end{align}

where we have collected the diagonal and off-diagonal terms separately. The
off-diagonal terms encode the temporal correlation between the measurement $k$
and $l$ and hence vanishes for uncorrelated data. At equilibrium, the diagonal
terms are independent of time "$k$" and hence $\sigma^2_{\mathcal{O}_k} =
\sigma^2_{\mathcal{O}}$. This simplifies the expression
\ref{eq:variance_estimator} to:

\begin{equation}
  \label{eq:variance_estimator_uncorrelated}
  \sigma^2_{\overline{\mathcal{O}}} = \sigma^2_{\mathcal{O}}/N
\end{equation}

The distribution of $\mathcal{O}_k$ is often Gaussian by the central limit
theorem atleast for weakly correlated data in asymptotic limit of
$N\rightarrow\infty$. The variance of the mean
$\sigma^2_{\overline{\mathcal{O}}}$ is the squared width of this Gaussian
distribution which is also called the statistical error of the estimator
$\overline{\mathcal{O}}$. This means that $68\%$ of the measurements
$\mathcal{O}_k$ will lie within one standard deviation $\sigma_{\mathcal{O}}$
from the mean $\overline{\mathcal{O}}$. 

For strongly correlated data, the off-diagonal terms in equation
\ref{eq:variance_estimator} are non-zero and the distribution of $\mathcal{O}_k$
is not Gaussian. Using the symmetry $k\leftrightarrow l$ in equation
\ref{eq:variance_estimator} we can rewrite the summation $\sum_{k\neq l}^{N}$ as
$2\sum_{k=1}^{N}\sum_{l=k+1}^{N}$. By reodering the terms and using time
translation invariance of the correlation function $A(k)$ we can obtain:

\begin{equation}
  \sigma^2_{\overline{\mathcal{O}}} = \frac{1}{N}\left[ \sigma^2_{\mathcal{O}} + 2\sum_{k=1}^{N} \left(\langle\mathcal{O}_k\mathcal{O}_{k+1}\rangle - \langle\mathcal{O}_k\rangle\langle\mathcal{O}_{k+1}\rangle\right) \left(1-\frac{k}{N}\right) \right]
\end{equation}

Factoring out $\sigma^2_{\mathcal{O}}$, we obtain:

\begin{equation}
  \label{eq:variance_estimator_correlated}
  \sigma^2_{\overline{\mathcal{O}}} = \frac{\sigma^2_{\mathcal{O}}}{N} 2\tau_{\mathcal{O}, exp}
\end{equation}

where $\tau_{\mathcal{O}, exp}$ is the integrated autocorrelation time defined
as:

\begin{align}
  \label{eq:autocorrelation_time}
  \tau_{\mathcal{O}, exp} &= \frac{1}{2} + \sum_{k=1}^{N} \frac{\langle\mathcal{O}_k\mathcal{O}_{k+1}\rangle - \langle\mathcal{O}_k\rangle\langle\mathcal{O}_{k+1}\rangle}{\sigma_{\mathcal{O}}^2} \left(1-\frac{k}{N}\right) \\
        &= \frac{1}{2} + \sum_{k=1}^{N} A(k) \left(1-\frac{k}{N}\right)
\end{align}

where $A(k)$ is the normalized autocorrelation function such that $A(0) = 1$.
For a proper simulation, we must choose $N$ such that $N>>\tau_{\mathcal{O},
exp}$ so that $A(k)$ is already exponentially small before the correction term
$\left(1-\frac{k}{N}\right)$ becomes significant. 

Equation \ref{eq:variance_estimator_correlated} implies that due to temporal
correlations in the data, the statistical error of the estimator $\epsilon =
\sqrt{\sigma^2_{\overline{\mathcal{O}}}}$ is increased by a factor of
$\sqrt{2\tau_{\mathcal{O}, exp}}$. This can be rephrased as the effective number
of measurements $N_{\text{eff}}$ being reduced by a factor of
$\sqrt{N_{\text{eff}}}$ as compared to uncorrelated data. The term
$\sqrt{N_{\text{eff}}}$ is called the effective sample size and is given by
$\sqrt{N_{\text{eff}}} = N/2\tau_{\mathcal{O}, exp}$. This implies that we
obtain uncorrelated data approximately every $2\tau_{\mathcal{O}, exp}$ time
steps.


\subsubsection{Bias in the Estimator}
As discussed above, the effective number of measurements $N_{\text{eff}}$ is
reduced by a factor of $\sqrt{2\tau_{\mathcal{O}, exp}}$ due to temporal
correlations in the data. Some quantites like specific heat $C$ are underestimed
if effective statistics becomes too small. The standard estimator for variance
is given by:

\begin{equation}
  \hat{\sigma_{e_i}^2} = \overline{\text{e}^2} - \overline{\text{e}}^2 = \frac{1}{N}\sum_{k=1}^{N}(e_k-\overline{e})^2
\end{equation}

where $e_k$ is the energy at time step $k$ and $\overline{e}$ is the mean
energy. To obtain the expectation value of the variance, we can subtract and add
$\langle\overline{e}\rangle^2$ as:

\begin{equation}
  \langle \hat{\sigma_{e_i}}^2 \rangle = \langle \overline{\text{e}^2} \rangle - \langle \overline{\text{e}} \rangle^2 - (\langle \overline{\text{e}} ^2\rangle - \langle \overline{\text{e}} \rangle^2 )
\end{equation}

The first two terms give $\sigma_{e_i}^2$ and the last terms gives
$\sigma_{\overline{e}_i}^2 = \sigma_{e_i}^2 2\tau_{\text{e, int}}/N$. This
results as follows:

\begin{equation}
  \langle \hat{\sigma_{e_i}}^2 \rangle = \sigma_{e_i}^2 \left(1 - \frac{2\tau_{\text{e, int}}}{N}\right) = \sigma_{e_i}^2 \left(1 - \frac{1}{N_{\text{eff}}}\right)
\end{equation}

The estimator $\hat{\sigma_{e_i}}^2$ is biased and underestimates the true
variance $\sigma_{e_i}^2$ by a factor of $1/N_{\text{eff}}$. This type of
estimator is called a weakly biased estimator.

Hence, we observe that for large autocorrelation times or small effective
statistics, the bias in the estimator $\hat{\sigma_{e_i}}^2$ becomes large.
Therefore, special care must be taken to ensure that the effective statistics
are large enough to obtain unbiased results especially in case of local update
algorithms.

